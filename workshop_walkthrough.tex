\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{parskip}
\usepackage[most]{tcolorbox}

\hypersetup{
  colorlinks=true,
  linkcolor=blue!60!black,
  urlcolor=blue!60!black
}

\newtcolorbox{promptbox}{
  colback=blue!5,
  colframe=blue!60!black,
  title=Prompt,
  width=\linewidth,
  boxsep=2mm,
  left=1mm,
  right=1mm,
  top=1mm,
  bottom=1mm
}

\title{LLM Coding Workflow Workshop\\\large 2-Hour Hands-On Walkthrough}
\author{Razvan Marinescu --- UC Santa Cruz}
\date{\today}

\begin{document}
\maketitle

\section*{Goal}
By the end of this session, each participant should be able to:
\begin{itemize}[leftmargin=1.2em]
  \item Use LLMs in \textbf{Cursor} to write new code and modify existing code.
  \item Use an \textbf{LLM CLI agent} (Codex or Claude CLI) for file/system operations.
  \item Build a small local data workflow: load CSV data, compute summaries, and generate plots.
  \item Produce a short document artifact (Markdown and optional LaTeX table) generated from local data.
\end{itemize}

\section*{Materials in this folder}
\begin{itemize}[leftmargin=1.2em]
  \item \texttt{data/students\_scores.csv}, \texttt{data/model\_runs.csv}
  \item \texttt{src/starter\_analysis.py}
  \item \texttt{src/text\_utils.py}
  \item \texttt{src/generate\_report.py}
  \item \texttt{src/plot\_metrics.py}
  \item \texttt{tests/test\_text\_utils.py}
  \item \texttt{requirements.txt}, \texttt{Makefile}
\end{itemize}

\section*{Session timeline (120 min)}
\begin{longtable}{p{0.15\linewidth}p{0.8\linewidth}}
\toprule
Time & Activity \\
\midrule
0--10 min & Environment checks + quick LLM sanity prompts in Cursor and CLI \\
10--35 min & Exercise A: write new code with LLM assistance (analysis function) \\
35--60 min & Exercise B: modify existing code + test-driven bug fixing \\
60--85 min & Exercise C: use CLI agent for file tasks and report generation \\
85--100 min & Exercise D: generate plots/charts from local CSV data \\
100--112 min & Exercise E: generate a stats table script and export LaTeX table \\
112--120 min & Exercise F: conference-paper LaTeX workflow with LLM debugging \\
\bottomrule
\end{longtable}

\section*{Exercise 0 --- Setup check (10 min)}
\textbf{Install uv (pick your OS):}
\textit{macOS (Homebrew)}
\begin{itemize}[leftmargin=1.2em]
  \item \verb|brew install uv|
\end{itemize}
\textit{Ubuntu / Linux}
\begin{itemize}[leftmargin=1.2em]
  \item \verb|curl -LsSf https://astral.sh/uv/install.sh | sh|
  \item \verb|source $HOME/.local/bin/env|
\end{itemize}
\textit{Check install}
\begin{itemize}[leftmargin=1.2em]
  \item \verb|uv --version|
\end{itemize}

\textbf{Then run in terminal:}
\begin{itemize}[leftmargin=1.2em]
  \item \verb|cd llm-coding|
  \item \verb|uv venv .venv|
  \item \verb|source .venv/bin/activate|
  \item \verb|uv pip install -r requirements.txt|
  \item \verb|uv run python src/starter_analysis.py|
  \item \verb|uv run python src/generate_report.py|
  \item \verb|uv run python src/plot_metrics.py|
\end{itemize}

\textbf{Quick prompts (Cursor and CLI):}
\begin{promptbox}
\begin{itemize}[leftmargin=1.2em]
  \item ``Read this folder and explain what each file is for.''
  \item ``Suggest 3 improvements to make this mini-project clearer for a beginner.''
\end{itemize}
\end{promptbox}

\section*{Exercise A --- Write new code in Cursor (25 min)}
Open \texttt{src/starter\_analysis.py}. There are TODOs in:
\begin{itemize}[leftmargin=1.2em]
  \item \texttt{compute\_group\_summary(...)}
  \item \texttt{find\_top\_improvers(...)}
\end{itemize}

\textbf{Task:}
\begin{enumerate}[leftmargin=1.4em]
  \item Ask Cursor to implement both TODOs.
  \item Ask Cursor to add docstrings and type hints if missing.
  \item Run: \verb|uv run python src/starter_analysis.py|
\end{enumerate}

\textbf{Prompt examples:}
\begin{promptbox}
\begin{itemize}[leftmargin=1.2em]
  \item ``Implement TODO functions in this file only. Keep output deterministic and concise.''
  \item ``Before editing, explain your plan in 3 bullets; after editing, show what changed.''
\end{itemize}
\end{promptbox}

\section*{Exercise B --- Modify code + tests (25 min)}
Open \texttt{src/text\_utils.py}. It contains intentionally weak behavior.

\textbf{Task:}
\begin{enumerate}[leftmargin=1.4em]
  \item Run tests: \texttt{uv run pytest -q} (or \texttt{make test})
  \item Use Cursor to fix failing tests in \texttt{tests/test\_text\_utils.py}
  \item Ensure all tests pass.
\end{enumerate}

\textbf{Prompt examples:}
\begin{promptbox}
\begin{itemize}[leftmargin=1.2em]
  \item ``Read failing tests and patch only src/text\_utils.py. Do not change tests unless absolutely necessary.''
  \item ``Keep functions simple and robust for edge cases (extra spaces, mixed case, punctuation).''
\end{itemize}
\end{promptbox}

\section*{Exercise C --- CLI agent workflow (25 min)}
Use Codex/Claude CLI to do a small multi-file task:
\begin{enumerate}[leftmargin=1.4em]
  \item Ask the CLI to inspect project structure.
  \item Ask it to run \texttt{uv run python src/generate\_report.py}.
  \item Ask it to improve \texttt{reports/workshop\_report.md} by adding one extra section:
  ``Potential failure modes in this dataset and mitigation ideas.''
\end{enumerate}

\textbf{Prompt template:}
\begin{promptbox}
\begin{enumerate}[leftmargin=1.4em]
  \item You are helping with a local Python project.
  \item Inspect files.
  \item Run report generation.
  \item Update the report with a short ``failure modes'' section.
  \item Do not add new dependencies.
  \item Show me exactly what you changed.
\end{enumerate}
\end{promptbox}

\section*{Exercise D --- Plots/charts from local data (20 min)}
Run: \verb|uv run python src/plot_metrics.py|

This should generate:
\begin{itemize}[leftmargin=1.2em]
  \item \texttt{reports/score\_by\_group.png}
  \item \texttt{reports/accuracy\_vs\_latency.png}
\end{itemize}

Move back to \textbf{Cursor}. In the agent chat, drag and drop
\texttt{src/plot\_metrics.py} into the prompt box before sending your prompt.

\textbf{Prompt example:}
\begin{promptbox}
\begin{itemize}[leftmargin=1.2em]
  \item ``Using the attached file, add one additional plot:
  token usage distribution by model. Save it as
  \texttt{reports/tokens\_by\_model.png}. Keep existing plots unchanged,
  add axis labels/title, and show exactly what changed.''
\end{itemize}
\end{promptbox}

% \textbf{Stretch task:}
Once it generated it, ask the LLM to run the script and check the output.

\section*{Exercise E --- Scripted stats table generation (12 min)}
In \textbf{Cursor}, ask the LLM to write a new script (for example:
\texttt{src/build\_stats\_table.py}) that reads local CSV data and writes a LaTeX table to
\texttt{reports/stats\_table.tex}.

\textbf{Task:}
\begin{enumerate}[leftmargin=1.4em]
  \item Students must specify the table structure (rows and columns) in the prompt.
  \item Table entries must report mean $\pm$ std for selected metrics.
  \item Add significance stars for p-values less than \texttt{0.5}.
  \item Save the final table as a standalone LaTeX snippet in \texttt{reports/stats\_table.tex}.
  \item Run the script and inspect the output file.
\end{enumerate}

\textbf{Prompt example:}
\begin{promptbox}
\begin{itemize}[leftmargin=1.2em]
  \item ``I attached \texttt{data/students\_scores.csv}. Create
  \texttt{src/build\_stats\_table.py} that generates \texttt{reports/stats\_table.tex} as well as an equivalent markdown table.
  Use rows=\{baseline, llm\_assisted\} and columns=\{assignment, midterm, final\}.
  Each cell should show mean $\pm$ std for that group/metric.
  Also compute a two-group p-value per metric and append stars to the metric header when
  p-value $< 0.5$. Use booktabs formatting. After editing, run the script and show exactly what changed.'' 
\end{itemize}
\end{promptbox}

After the LLM finishes creating the script, ask it to run the script and inspect the output file. 

\section*{Exercise F --- Conference-paper LaTeX workflow (8 min)}
Start from an online conference template (for example NeurIPS) and ask the LLM to set up
a minimal paper draft locally.

\textbf{Task:}
\begin{enumerate}[leftmargin=1.4em]
  \item Ask the LLM to create a new folder \texttt{paper/}.
  \item Ask it to retrieve the NeurIPS style files from
  \url{https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles}
  and save the \texttt{.tex} and \texttt{.sty} files into \texttt{paper/}.
  \item Ask the LLM to compile LaTeX in \texttt{paper/}.
  \item Then ask the LLM to include:
  \begin{itemize}[leftmargin=1.2em]
    \item plots from \texttt{reports/} (including \texttt{tokens\_by\_model.png}),
    \item table from \texttt{reports/stats\_table.tex}.
  \end{itemize}
  \item Re-compile and verify the PDF builds.
\end{enumerate}

\textbf{Prompt examples (run as two separate prompts):}
\textbf{Prompt 1 (setup + first compile):}
\begin{promptbox}
\begin{itemize}[leftmargin=1.2em]
  \item ``Create a new folder \texttt{paper/}. From
  \url{https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles},
  download/save the NeurIPS \texttt{.tex} template and \texttt{.sty} style file
  into \texttt{paper/}. Then compile with \texttt{pdflatex} and show me the files created.''
\end{itemize}
\end{promptbox}

After Prompt 1 finishes and \texttt{pdflatex} succeeds, run Prompt 2.

\textbf{Prompt 2 (add figures + table):}
\begin{promptbox}
\begin{itemize}[leftmargin=1.2em]
  \item ``Now edit the paper in \texttt{paper/} to include:
  \texttt{../reports/score\_by\_group.png},
  \texttt{../reports/accuracy\_vs\_latency.png},
  \texttt{../reports/tokens\_by\_model.png},
  and the table from \texttt{../reports/stats\_table.tex}.
  Re-compile and show exactly what changed.''
\end{itemize}
\end{promptbox}

\section*{Best practices to emphasize}
\begin{itemize}[leftmargin=1.2em]
  \item Ask for a plan first, then ask for edits.
  \item Scope the edit: file(s), constraints, and expected output.
  \item Require verification steps (tests, script runs, lint checks).
  \item Prefer iterative prompts over one giant prompt.
  \item Keep humans in the loop for correctness, reproducibility, and security.
\end{itemize}

\section*{Common failure modes to discuss}
\begin{itemize}[leftmargin=1.2em]
  \item Silent hallucinations (invented functions, files, APIs).
  \item Over-broad code edits when prompt scope is vague.
  \item Incomplete environment assumptions (wrong Python/package versions).
  \item Data leakage/privacy risks when sharing sensitive content.
  \item Overconfidence in generated analysis without validation.
\end{itemize}

\section*{Wrap-up checklist}
\begin{itemize}[leftmargin=1.2em]
  \item[] [ ] I can use Cursor to create and edit multi-file code changes.
  \item[] [ ] I can use an LLM CLI agent to run commands and modify files safely.
  \item[] [ ] I can generate and validate local data plots and summaries.
  \item[] [ ] I understand at least 3 failure modes and mitigation strategies.
\end{itemize}

\vspace{1em}
\noindent\textbf{Optional extension for next session:} multi-agent orchestration where one agent writes code, one runs tests, and one performs review.

\end{document}

